{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación de Modelos de Clasificación de Sentimientos\n",
    "\n",
    "Este notebook compara modelos tradicionales de ML vs redes neuronales para clasificación de sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports y configuración\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.preprocessing import preprocess_dataframe\n",
    "from src.traditional.models import train_and_compare_models, TraditionalClassifier\n",
    "from src.neural.models import train_and_compare_neural_models, NeuralClassifier\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carga y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "df = pd.read_csv('../data/respuestas_cafeteria.csv')\n",
    "print(f\"Total de registros: {len(df)}\")\n",
    "print(f\"\\nDistribución de clases:\")\n",
    "print(df['kind_of_comment'].value_counts())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar texto\n",
    "df = preprocess_dataframe(df, text_column='comment', output_column='clean_comment')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir X e y\n",
    "X = df['clean_comment']\n",
    "y = df['kind_of_comment']\n",
    "\n",
    "print(f\"Total de muestras: {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelos Tradicionales (Logistic Regression, SVM, Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar y comparar modelos tradicionales\n",
    "traditional_results, (X_train, X_test, y_train, y_test) = train_and_compare_models(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar matrices de confusión - Modelos Tradicionales\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (model_name, data) in enumerate(traditional_results.items()):\n",
    "    cm = data['results']['confusion_matrix']\n",
    "    labels = data['classifier'].model.classes_ if hasattr(data['classifier'].model, 'classes_') else ['negativo', 'neutro', 'positivo']\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    axes[idx].set_title(f\"{model_name.upper()}\\nAccuracy: {data['results']['accuracy']:.4f}\")\n",
    "    axes[idx].set_xlabel('Predicción')\n",
    "    axes[idx].set_ylabel('Real')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/traditional_confusion_matrices.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelos de Redes Neuronales (FNN, CNN, LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar y comparar redes neuronales\n",
    "neural_results, splits = train_and_compare_neural_models(\n",
    "    X, y, \n",
    "    epochs=30,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar curvas de aprendizaje\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (model_name, data) in enumerate(neural_results.items()):\n",
    "    history = data['history']\n",
    "    \n",
    "    axes[idx].plot(history['accuracy'], label='Train')\n",
    "    if 'val_accuracy' in history:\n",
    "        axes[idx].plot(history['val_accuracy'], label='Validation')\n",
    "    \n",
    "    axes[idx].set_title(f\"{model_name.upper()} - Accuracy: {data['results']['accuracy']:.4f}\")\n",
    "    axes[idx].set_xlabel('Época')\n",
    "    axes[idx].set_ylabel('Accuracy')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/neural_learning_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparación Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla comparativa\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, data in traditional_results.items():\n",
    "    comparison_data.append({\n",
    "        'Modelo': model_name.upper(),\n",
    "        'Tipo': 'Tradicional',\n",
    "        'Accuracy': data['results']['accuracy']\n",
    "    })\n",
    "\n",
    "for model_name, data in neural_results.items():\n",
    "    comparison_data.append({\n",
    "        'Modelo': model_name.upper(),\n",
    "        'Tipo': 'Red Neuronal',\n",
    "        'Accuracy': data['results']['accuracy']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('Accuracy', ascending=False)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMPARACIÓN FINAL DE MODELOS\")\n",
    "print(\"=\"*50)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de barras comparativo\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#3498db' if t == 'Tradicional' else '#e74c3c' for t in comparison_df['Tipo']]\n",
    "\n",
    "bars = plt.bar(comparison_df['Modelo'], comparison_df['Accuracy'], color=colors)\n",
    "\n",
    "# Añadir valores sobre las barras\n",
    "for bar, acc in zip(bars, comparison_df['Accuracy']):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title('Comparación de Accuracy por Modelo', fontsize=14)\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "# Leyenda\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#3498db', label='Tradicional'),\n",
    "                   Patch(facecolor='#e74c3c', label='Red Neuronal')]\n",
    "plt.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../models/model_comparison.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el mejor modelo de cada tipo\n",
    "best_traditional = max(traditional_results.items(), key=lambda x: x[1]['results']['accuracy'])\n",
    "best_neural = max(neural_results.items(), key=lambda x: x[1]['results']['accuracy'])\n",
    "\n",
    "print(f\"Mejor modelo tradicional: {best_traditional[0]} ({best_traditional[1]['results']['accuracy']:.4f})\")\n",
    "print(f\"Mejor modelo neuronal: {best_neural[0]} ({best_neural[1]['results']['accuracy']:.4f})\")\n",
    "\n",
    "# Guardar modelos\n",
    "best_traditional[1]['classifier'].save(f'../models/best_traditional_{best_traditional[0]}.joblib')\n",
    "best_neural[1]['classifier'].save(f'../models/best_neural_{best_neural[0]}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusiones\n",
    "\n",
    "Resumen de hallazgos:\n",
    "- **Mejor modelo tradicional**: [completar tras ejecución]\n",
    "- **Mejor red neuronal**: [completar tras ejecución]\n",
    "- **Recomendación**: [completar según resultados]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
